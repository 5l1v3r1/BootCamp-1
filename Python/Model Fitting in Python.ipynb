{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting in Python: Scipy, SciKit-Learn, and Emcee\n",
    "\n",
    "This is a tutorial on scientific Python for the [KIPAC computing boot camp](http://kipac.github.io/BootCamp).\n",
    "\n",
    "Authors: [Sean McLaughlin](http://github.com/mclaughlin6464), [Joe Derose](http://github.com/j-dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting is one of the most common and important things researchers have to do. There is not a one-size-fits-all solution to all model fitting problems, however. For example:\n",
    "* Perhaps we need to a fit a relatively simple model to a large dataset (or a large number of smaller datasets) and we want the fit to be efficient\n",
    "* Maybe we don't know what kind of model we need so we want to be able to easily compare a suite of models\n",
    "* Perhaps the model fit is our entire analyis, and we need to understand \n",
    "\n",
    "We will look at Python packages that handle different situations like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First though, we'll need some data to fit! We'll generate two datasets, a simple linear dataset and a slightly more complex nonlinear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division # if you don't have Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_m, true_b = 0.68, 2.38\n",
    "N = 100\n",
    "x = np.linspace(0,50,N)\n",
    "\n",
    "def linear_func(x, m=true_m, b=true_b):\n",
    "    return m*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_linear_y = linear_func(x)\n",
    "plt.plot(x, true_linear_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add noise\n",
    "sigma = 1.8\n",
    "obs_linear_y = true_linear_y+ np.random.randn(N)*sigma\n",
    "plt.errorbar(x, obs_linear_y, yerr = np.ones(N)*sigma,marker='o', ls='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_p1, true_p2, true_p3, true_p4, true_p5 = -2.1, 0.8, -0.021, 4.3, 0.6\n",
    "true_nonlinear_params = np.array([true_p1, true_p2, true_p3, true_p4, true_p5])\n",
    "def nonlinear_func(x,p1=true_p1,p2=true_p2,p3=true_p3,p4=true_p4,p5=true_p5):\n",
    "    # define a complex nonlinear function to fit\n",
    "    return p1+p2*x+p3*x*x+p4*np.sin(x*p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_nonlinear_y = nonlinear_func(x)\n",
    "plt.plot(x, true_nonlinear_y, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add noise\n",
    "obs_nonlinear_y = true_nonlinear_y+ np.random.randn(N)*sigma\n",
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o', ls='None', color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy\n",
    "----\n",
    "NumPy contains much of the functionality required to perform efficient operations in python, but you will soon realize that there is not a great deal of high level functionality implemented in numpy. \n",
    "\n",
    "[Scipy](http://docs.scipy.org/doc/scipy/reference/) is the module where you *will* find a great deal of high level functionality that is very useful for day to day scientific computing. \n",
    "\n",
    "Some of the very useful submodules that can be found in SciPy are:\n",
    "\n",
    "* [Linear Algebra](http://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html)\n",
    "* [Statistics](http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html)\n",
    "* [Integration](http://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html)\n",
    "* [Interpolation](http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html)\n",
    "* [FFT](http://docs.scipy.org/doc/scipy/reference/tutorial/fftpack.html)\n",
    "* [Optimization](http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)\n",
    "\n",
    "Today we be delving into some of the details of Linear Algebra and Optimization, but keep in mind that you will find similar depth in these other applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a simple linear system. Scipy has a great many useful linear algebra functions, including inverses, determinants, and eigenvalues! We can showcase some of these with a dummy matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.eye(4)\n",
    "A[1,2] = 3 # just to keep it interesting\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import inv, det, eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(inv(A), det(A), eigvals(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on the linear least squares solver in scipy. Recall that linear least squares finds the solution $\\hat{x}$ to the system\n",
    "\n",
    "$$ \\mathbf{A}x = \\mathbf{b} $$ \n",
    "\n",
    "that minimizes the squared absolute error. \n",
    "\n",
    "$$ \\left\\lVert \\mathbf{A}x - b \\right \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the special case that your data satisfies the [Gauss-Markov Theroem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) (which ours do) the linear least squares solution will give the optimal unbiased estimator $\\hat{x}$. Let's try it out on our linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "fit_m, fit_b, r,p, std_err = linregress(x, obs_linear_y)\n",
    "print('R^2 = ',r**2)\n",
    "print(fit_m, fit_b)\n",
    "print(true_m, true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error is not an estimate of the error on the data points; it is used for estimating uncertanties in parameter estimates. To estimate sigma one can use this formula ( I found on Wikipedia)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.sqrt(np.sum((obs_linear_y - linear_func(x, fit_m, fit_b))**2)/(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, obs_linear_y, yerr = np.ones(N)*sigma,marker='o', ls='None')\n",
    "plt.plot(x, fit_m*x+fit_b, color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually used `scipy.stats`'s `linregress` module here instead of `lstsq`. That will give us the same answer, but with a few extra steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.ones((N,2))\n",
    "A[:,1] = x #build a design matrix\n",
    "\n",
    "(fit_b2, fit_m2), _,_,_ = lstsq(A, obs_linear_y)\n",
    "print(fit_m2, fit_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the extra steps? Well, that's because **linear least squares is not just for fitting lines**! Any linear model can be fit with linear least squares. Let's try that for our next excercise! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Use linear least squares to fit to our nonlinear model. You'll have to assume that we know `true_p5` perfectly; all other parameters are linear. You'll have to start by making a design matrix, which will be similar to the matrix A above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear least squares works well for linear problems (which is not just lines!) But of course this is not all problems. Indeed, even for Excercise 1, we had to assume we knew a nonlinear parameter. How would we go about fitting for parameters like these? \n",
    "\n",
    "This is in general a much harder than linear optimization. If you have a consistent solution that works in all cases let me know and maybe we can share a Fields medal!\n",
    "\n",
    "We will focus on two techniques that are commonly used in scipy: `curve_fit` (which uses [nonlinear least squares](http://mathworld.wolfram.com/NonlinearLeastSquaresFitting.html) )and `minimize` (which uses a variety of gradient descent-esque methods). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit, minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html), which has an easy to use interface. To use curve fit, we'll have to a defince a function (which we've already done) and give an initial guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_guess = np.array([-1, 1, 0, 3, 0.4])\n",
    "popt, pcov = curve_fit(nonlinear_func, x, obs_nonlinear_y, p0=initial_guess)\n",
    "print(popt)\n",
    "print(true_nonlinear_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "plt.plot(x, nonlinear_func(x, *popt), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, this did ok, but not great. This is hard! In general you may want to try several guesses if you're unsatisfied. In this case, since we know the truth, we can make an unreasonably good guess. We can also add some uncertainty information that we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_guess = np.array([-2, 1, 0, 4, 0.55])\n",
    "popt, pcov = curve_fit(nonlinear_func, x, obs_nonlinear_y, p0=initial_guess, sigma=np.ones(N)*sigma)\n",
    "print(popt)\n",
    "print(true_nonlinear_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "plt.plot(x, nonlinear_func(x, *popt), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also got a covariance matrix from the estimator. Let's use it now to calculates some uncertanties on our params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = np.sqrt(np.diag(pcov))\n",
    "for i,(p, e) in enumerate(zip(popt, errors)):\n",
    "    print('Param {} = {} +/-{}'.format(i+1,p,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move onto [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize). Minimize has a bit of a different use case; it seeks to minimize a function, rather than find a best fit. However, if we phrase our problem as a minimization, we can use it! Minimize supports several different optimization algorithms, depending on the nature of the problem. If you have access to bounds, constraints, or jacobian/hessians, you should use a function that exploits them. For now, we'll focus on the more basic ones. \n",
    "\n",
    "We'll begin by designing a **loss function**, which represents the loss that we want to minimize. We'll return to these again when we discuss MCMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_function(params, func, y_data, x_data):\n",
    "    # just stick with a simple least squares\n",
    "    return np.sum((y_data - func(x_data, *params))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_guess = np.array([-1, 1, 0, 3, 0.4])\n",
    "res = minimize(loss_function, initial_guess, args = (nonlinear_func, obs_nonlinear_y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimze returns an object with results. We'll check if it thinks it was successful and see what the best guess was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(res.success)\n",
    "print(res.x)\n",
    "print(true_nonlinear_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "plt.plot(x, nonlinear_func(x, *res.x), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn. No luck. \n",
    "\n",
    "There's a lot of ways we could proceed here. Why don't we try it as an excercise? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Find a good fit to the nonlinear data using minimize. You may want to try using a different method, or perhaps doing something fancy with your loss function (for example, regularizing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn \n",
    "---\n",
    "[scikit-learn](http://scikit-learn.org/stable/) is a massive and very powerful library for machine learning and data analysis. Like with `scipy` we will delve into the details of one module (Regression) to get a taste of what it holds. \n",
    "\n",
    "The most powerful feature of sklearn is not just the function contained within, but the way they are designed. Almost all objects in sklearn use the same API design. This means you can easily test several different models in the same notebook! I'll import a few more or less random series of methods and make fits with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "#model = GaussianProcessRegressor(alpha = sigma, normalize_y=True)\n",
    "#model = RandomForestRegressor(n_estimators = 10)\n",
    "#model = KernelRidge(kernel='rbf')\n",
    "\n",
    "A = np.c_[np.ones(N), x]\n",
    "model.fit(A,obs_linear_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, obs_linear_y, yerr = np.ones(N)*sigma,marker='o',color='b', ls='None')\n",
    "plt.plot(x, model.predict(A), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the above two cells by commenting out different models. How does the prediction change?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, for such a large suite of models, we need a reliable way to decide which one to use! One place to start is by looking at various quantitative scores. Things like R^2 or Mean Square Error give an idea of which model is a better fit, better than \"eyeballing\" it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#R^2 score\n",
    "model.score(A, obs_linear_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(obs_linear_y, model.predict(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's obvious that this is not ideal; in many cases the model that fits \"better\" is overfit! Is there anyway we can see if our fit generalizes? \n",
    "\n",
    "We can use cross validation, which sklearn also supports. There are functions for generating separate test and validation sets easily. We'll use `cross_val_score`, which does this behind the scenes and calculates the goodness of fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(-1*cross_val_score(model, A, obs_linear_y, cv = 10, scoring='neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only the beginning of such features. Some of our models above have *hyperparameters* which also need to be optimized. We won't explore any functions for that here, but needless to say sklearn has helper functions for that as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Repeat the above process for the nonlinear data. What model works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC and Emcee\n",
    "----\n",
    "In some types of problems, we care about more than just the \"best fit\" to a model. In cosmology especially, the distribution of what parameters are likely is more important. What are the likely values of a parameter? Are they degenerate/correlated? These are the kind of questions that Bayesian Analysis (and specifically, MCMC) can answer.\n",
    "\n",
    "This is an extremely in depth-topic that can barely be covered here. However, I hope that we can scratch the surface of why this is something you'll consider using in your analysis. For a great tutorial on these topics in a similar format, read [Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to understand the distribution of some parameters $\\theta$ given some data $x$. Bayes' theorem states that the following is true:\n",
    "\n",
    "$$ P(\\theta | x) = \\frac{P(\\theta)\\;P(x | \\theta)}{P(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attach a little vocab to the terms here. $P(\\theta | x)$, what we're after, is known as the *posterior*. $P(\\theta)$ is known as the *prior* and represents our prior knowledge (if any) about the parameters in our model. $P(x | \\theta)$ is the *likelihood* of the data given the parameters. Finally, $P(x)$ is known as the *evidence*. In general, the evidence is difficult to compute. Fortunately, we usually don't need to calculate it, as it serves only as a normalization constant. Therefore, we can refer to the posterior as\n",
    "\n",
    "$$ P(\\theta|x) \\propto P(\\theta)\\;P(x|\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few things we'll want to do with the posterior. Firstly, we'll want to draw samples from it. The density of these samples will be proportional to the posterior distribution, if we use MCMC. Additionally, we may want to integrate over some parameters. Perhaps there are some parameters we just don't care about. A good example is sigma, which we've presumed we've known up til now, but in many real cases we don't know well. We'll want to integrate over those, which will incorporate any uncertantiy in those parameters into our final distribution. We also want to know the expectation values of individual parameters after integrating over all the others (wheter we care about them or not). MCMC enables us to do all of this! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using [emcee](http://dfm.io/emcee/current/) which is an MCMC sampler written by astonomer. If you don't have it already, it can be installed with `pip install emcee`. Let's start by fitting to our linear model. We'll have to build a few basic things to hand to the emcee sampler, so let's start there. We'll need:\n",
    "* A prior over our parameters\n",
    "* A liklihood for the parameters given our data\n",
    "* Some hyperparameters for the sampler that determine its sampling scheme\n",
    "* An initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    b,m,sigma = theta\n",
    "    if sigma < 0:\n",
    "        return -np.inf\n",
    "    return 1/(sigma*np.sqrt(1+m**2)**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an uniformative prior on all our parameters. For a justification, read [this post](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/) about it. For what it's worth, in this problem a flat prior in both parameters would probably be fine. note that since many of liklihoods we'll be dealing with are so small, \n",
    "the prior and the liklihood are actually the lnprior, lnlikelihood.\n",
    "Additionally, since we only care about proportional liklihoods and priors,we can be lazy about the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnlike(theta, x, y):\n",
    "    b,m,sigma = theta\n",
    "    y_pred = linear_func(x,m,b)\n",
    "    return -np.sum(2*np.log(sigma) + ((y - y_pred)/sigma)**2)\n",
    "\n",
    "args = (x, obs_linear_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the log of a Gaussian, but with a few terms that won't change the final product because of their proportionality. Now, let's take care of the rest of the bookkeeping and run the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, *args):\n",
    "    p = lnprior(theta)\n",
    "    if not np.isfinite(p):\n",
    "        return -np.inf\n",
    "    return p+lnlike(theta, *args)\n",
    "\n",
    "ndim = 3\n",
    "nwalkers = 100\n",
    "nsteps = 1100\n",
    "nburn = 100\n",
    "\n",
    "#Create a sampler with 100 walkers, each who take 1100 steps, of which\n",
    "# we will toss the first 100 (the \"burn in\")\n",
    "initial_guess = np.zeros((nwalkers, ndim))\n",
    "for i in xrange(nwalkers):\n",
    "    initial_guess[i,0] = np.random.rand()*20-10 #U(-10,10)\n",
    "    initial_guess[i,1] = np.random.randn()*2+1  #N(1, 2)\n",
    "    initial_guess[i,2] = np.random.rand()*5     #U(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now let's go\n",
    "import emcee as mc\n",
    "sampler = mc.EnsembleSampler(nwalkers, ndim, lnprob, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.reset()\n",
    "sampler.run_mcmc(initial_guess, nsteps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the sampler object has a chain attached to it. We can take a look and see the paths the walkers took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in xrange(nwalkers):\n",
    "    plt.plot(sampler.chain[w,:,0], alpha = 0.5)\n",
    "\n",
    "plt.ylabel(r'$b$')\n",
    "plt.xlabel('Steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks converged! In a real analysis you'd want to do a more nuanced analysis of the convergence of the chains, by taking a look at convergence statistics or something like that. For now, we'll trust our gut. Let's look at the posteriors of these parameters. We'll use the handy tool [ChainConsumer](https://samreay.github.io/ChainConsumer/chain_api.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "c = ChainConsumer()\n",
    "c.add_chain(sampler.chain[:,nburn:,:].reshape((-1, ndim))\\\n",
    "            , parameters = [r'$b$',r'$m$', r'$\\sigma$'],\n",
    "            walkers=nwalkers)\n",
    "fig = c.plotter.plot(truth = [true_b, true_m, sigma])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at only the non-nuisance parameters (marginalize out uncertainty)\n",
    "fig = c.plotter.plot(figsize=(5,5),parameters = [r'$b$', r'$m$'],\\\n",
    "                     truth = [true_b, true_m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.plotter.plot_distributions(truth=[true_b, true_m, sigma]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can look at some convergence statistics\n",
    "c.diagnostic.gelman_rubin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are a bit biased, but we've made fewer assumptions then above. We now also have a more complete understanding of the details of our model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "You probably know the drill by now. Can you perform a similar fit to our nonlinear model? This will be more complex then anything we've done so far; but the results will be interesting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exercise 1\n",
    "A = np.ones((N,4))\n",
    "A[:,1] = x\n",
    "A[:,2] = x**2\n",
    "A[:,3] = np.sin(true_p5*x)\n",
    "\n",
    "(fit_p1, fit_p2, fit_p3, fit_p4), _,_,_ = lstsq(A, obs_nonlinear_y)\n",
    "\n",
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "plt.plot(x, nonlinear_func(x, fit_p1, fit_p2, fit_p3, fit_p4), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exercise 2\n",
    "# I found that Powell gave a good fit\n",
    "initial_guess = np.array([-1, 1, 0, 3, 0.4])\n",
    "res = minimize(loss_function, initial_guess,method='Powell', args = (nonlinear_func, obs_nonlinear_y, x))\n",
    "print(res.x)\n",
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "plt.plot(x, nonlinear_func(x, *res.x), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#alternatively, one could try regularizing the last parameter, as it seems to dominate this fit. \n",
    "# Weirdly, I find it fits the data well despite not being \"right\"\n",
    "def loss_function2(params, func, y_data, x_data, l=-1e3):\n",
    "    # add in a l1 regularization\n",
    "    return np.sum((y_data - func(x_data, *params))**2)+l*params[-1]\n",
    "\n",
    "res = minimize(loss_function2, initial_guess, args = (nonlinear_func, obs_nonlinear_y, x))\n",
    "print(res.x)\n",
    "plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "plt.plot(x, nonlinear_func(x, *res.x), color = 'g', lw = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exercise 3\n",
    "# Feel free to throw in other estimators! \n",
    "from sklearn.metrics import r2_score\n",
    "for m, hp in zip([LinearRegression, GaussianProcessRegressor, RandomForestRegressor, KernelRidge],\\\n",
    "                 [{},{'alpha':sigma, 'normalize_y':True}, {'n_estimators':10}, {'kernel':'rbf'}]):\n",
    "    model = m(**hp)\n",
    "    print(model)    \n",
    "    A = np.c_[np.ones(N), x]\n",
    "    model.fit(A,obs_nonlinear_y)\n",
    "    plt.errorbar(x, obs_nonlinear_y, yerr = np.ones(N)*sigma,marker='o',color='r', ls='None')\n",
    "    plt.plot(x, model.predict(A), color = 'g', lw = 3)\n",
    "    print(r2_score(obs_nonlinear_y,model.predict(A)))\n",
    "    print(-1*cross_val_score(model, A, obs_nonlinear_y, cv = 10, scoring='neg_mean_squared_error'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "#I haven't worked this out yet! I leave it as an exercise to the reader :). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hodemulator]",
   "language": "python",
   "name": "conda-env-hodemulator-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
